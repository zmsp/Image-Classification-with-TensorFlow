{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Radiomics - 1p/19q Chromosome Status Classification Using Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Special thanks to:**  \n",
    "- Dr. Bradley J. Erickson M.D., Ph.D. - Department of Radiology - Mayo Clinic\n",
    "- Panagiotis Korfiatis, Ph.D. - Department of Radiology - Mayo Clinic\n",
    "- Dr. Daniel LaChance, M.D. - Department of Neurology - Mayo Clinic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Diagnosis of a brain tumor can be devastating to patients. Surgery is an essential step in management, and can be painful and life-threatening. Often the next step is radiation and chemotherapy which also has side effects and is expensive. Targeted therapies based on genetic properties of the tumor can improve response and reduce the side effects of treatment.  Thanks to work being performed at Mayo Clinic, new approaches using deep learning techniques to detect biomarkers can lead to more effective treatments and yield better health outcomes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Radiogenomics refers to the correlation between cancer imaging features and gene expression. Radiogenomics can be used to create biomarkers that identify the genomics of a disease without the use of an invasive biopsy. A biomarker is simply an indicator of some biological state or condition. Four biomarkers that appear important for brain tumors include 1p/19q chromosome co-deletion; MGMT-promoter methylation; IDH-1 mutation; and TERT. The focus of this lab is detection of 1p/19q co-deletion using deep learning, using convolutional neural networks. What is remarkable about this research is the novelty and promising results of combining deep learning with Radiogenomics. In addition to avoiding invasive biopsies, the detection or absence of biomarkers is significant because certain treatments of brain tumors are more effective in the presence or absence of a biomarker. Biomarker detection can ensure patients receive the most effective treatment for their particular scenario.\n",
    "\n",
    "Research performed shows that the detection of 1p/19q co-deletion using deep learning works well with T2 images compared to T1 post contrast images. Additionally, fewer layers and fewer neurons per layer produced better results while limiting overfitting of the training data. The model you will build consists of 14 layers.  Kernels (convolutions) were found to yield positive results when the convolutions were smaller - 3 x 3 - since these smaller convolutions could capture some of the finer details of the edges.  Originally the model was created using Keras and Theano framework, but we have altered the code slightly so that the TensorFlow framework could be used.  Tensorflow has great multi-GPU support - a necessity when working with large training datasets and complex neural networks - hence, the change in framework.\n",
    "\n",
    "The lab instructions that follow will guide you through one approach to establishing your own convolutional neural network to detect 1p/19q co-deletion biomarker. You will likely experiment with alterations to the network architecture, hyper-parameters and training data. We encourage you to do such experimentation and to utilize deep learning for the detection of other biomarkers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started\n",
    "Before going into details on architecting and training a neural network, there are some housekeeping tasks that need to be executed first.  Examples of such tasks include: \n",
    "- Importing libraries\n",
    "- Locking GPUs to prevent contention issues (if using Theano)\n",
    "- Changing channel order between frameworks (not shown)\n",
    "\n",
    "The above list reflects only some of the basic tasks you may need to perform at your site.\n",
    "\n",
    "Go ahead and place your cursor in each of the three code boxes below and click the \"run cell\" button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('I am process:')\n",
    "print (os.getpid())\n",
    "\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "\n",
    "from keras.layers import Conv2D, MaxPooling2D, Input\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.core import Flatten, Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from keras.utils import np_utils\n",
    "from _loadcsvdeep import load_set\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, auc\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "\n",
    "from keras.layers.core import ActivityRegularization\n",
    "from keras_diagram import ascii\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import math\n",
    "seed = 7\n",
    "\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture \n",
    "The code block that follows defines our network architecture and provides code that visualizes the model's performance.  Notice how easy it is to define a 14-layer convolution neural network!  Lets take a closer look at some of the layers in the network architecture by exploring the Keras / TensorFlow options used.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Sequential()**\n",
    "\n",
    "Immediately after the def cnn_model line below we see the following code: \n",
    "\n",
    "*model = Sequential()*\n",
    "\n",
    "Sequential() is a Keras specification that allows us to create a linear stack of layers in our neural network architecture.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Conv2D()**\n",
    "\n",
    "*model.add(Conv2D(16, (3, 3),activation='linear',kernel_initializer='he_uniform'))*\n",
    "\n",
    "Kernels act like filters.  Each kernel slides over an input image / space to yield a value that projects to a new space (see picture below).  As noted earlier, smaller convolutions offered better results because the smaller convolutions capture some of the finer details of the edges of a tumor.  All convolutional layers in this network use 3 X 3 kernels.  The architecture starts with 16 kernels in a layer, moves to using 32 kernels per layer, then 64 kernels per layer, and reaches a high of 96 kernels per layer.\n",
    "\n",
    "One of the major advantages of using deep neural networks is that you do not have to define the features yourself.  Convolutions find features in the data for you.  Each kernel needs an initial set of weights. There are numerous approaches to initializing these weights - 'he_uniform' was chosen here.  Feel free to experiment using other weight initilization options.\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"Convolution.png\" width=\"400\" height=\"300\">\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**LeakyReLU()**\n",
    "\n",
    "*model.add(LeakyReLU(alpha=.01))*\n",
    "\n",
    "LeakyReLU is one of many activation functions (sigmoid, tanh, ReLU, etc.) that can be selected.  Valuable information resides in the negative values when it comes to Radiogenomics.  As such, LeakyReLU was chosen as the activation function since negative values are retained and avoid saturation concerns sometimes experienced when using tanh.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**MaxPooling2D()**\n",
    "\n",
    "*model.add(MaxPooling2D(pool_size=(2, 2)))*\n",
    "\n",
    "Pooling is a down-sampling technique that reduces the number of computations that must be performed.  Maxpooling identifies the most important feature in a specified area (2 X 2 given the example above) and projects the maximum value onto a new space.  This model initially uses 2 X 2 maxpooling and then uses 7 X 7 maxpooling.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Flatten()**\n",
    "\n",
    "*model.add(Flatten())*\n",
    "\n",
    "Flattens the input. Does not affect the batch size.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Dense()**\n",
    "\n",
    "*model.add(Dense(512))*\n",
    "\n",
    "Dense reduces the number of neurons in a layer.  Dense is used twice to bring the number of neurons down to two just before the softmax layer to reflect what will be the binary prediction of either 'co-deletion' or 'no co-deletion'.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**GaussianNoise()**\n",
    "\n",
    "*model.add(keras.layers.noise.GaussianNoise(0.3))*\n",
    "\n",
    "To prevent overfitting of the training data, noise was intentionally added.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Dropout()**\n",
    "\n",
    "*model.add(Dropout(0.5))*\n",
    "\n",
    "Dropout represents yet another means of preventing or limiting overfitting of your model to the training data.  Dropout randomly avoids neurons during the forward and backward propogation phases.  The number of neurons not updating depends upon the value specified inside Dropout().  In this example, half (0.5) of the neurons are skipped.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Activation('softmax')**\n",
    "\n",
    "*model.add(Activation('softmax'))*\n",
    "\n",
    "The last layer in our network, the output layer, uses a softmax classifier to include the probability of each of the binary outcomes - 'co-deletion' and 'no co-deletion'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cnn_model(img_rows, img_cols, img_channels):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(16, (3, 3),activation='linear',kernel_initializer='he_uniform',\n",
    "                     input_shape=(img_rows, img_cols, img_channels)))\n",
    "    model.add(LeakyReLU(alpha=.01))   # add an advanced activation\n",
    "    model.add(Conv2D(16, (3, 3),activation='linear',kernel_initializer='he_uniform'))\n",
    "    model.add(LeakyReLU(alpha=.01))   # add an advanced activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2)) # Avoid over-fitting \n",
    "    model.add(Conv2D(32, (3, 3),activation='linear',kernel_initializer='he_uniform'))\n",
    "    model.add(LeakyReLU(alpha=.01))   # add an advanced activation\n",
    "    model.add(Conv2D(32, (3, 3),activation='linear',kernel_initializer='he_uniform'))\n",
    "    model.add(LeakyReLU(alpha=.01))   # add an advanced activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.2)) # Avoid over-fitting \n",
    "    model.add(Conv2D(64, (3, 3),activation='linear',kernel_initializer='he_uniform'))\n",
    "    model.add(LeakyReLU(alpha=.01))   # add an advanced activation\n",
    "    model.add(Conv2D(64, (3, 3),activation='linear',kernel_initializer='he_uniform'))\n",
    "    model.add(LeakyReLU(alpha=.01))   # add an advanced activation\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(128, (3, 3),activation='linear',kernel_initializer='he_uniform'))\n",
    "    model.add(LeakyReLU(alpha=.01))   # add an advanced activation\n",
    "    model.add(Conv2D(128, (3, 3),activation='linear',kernel_initializer='he_uniform'))\n",
    "    model.add(LeakyReLU(alpha=.01))   # add an advanced activation\n",
    "    model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "    model.add(keras.layers.noise.GaussianNoise(0.6))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=.01))   # add an advanced activation\n",
    "    model.add(Dropout(0.7)) # Avoid over-fitting  \n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.gray,savepdf='output.pdf'):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    f=plt.figure()\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    \n",
    "#   plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, ('%.2f' %cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"red\" if cm[i, j] > thresh else \"white\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    f.savefig(savepdf)\n",
    "   \n",
    "    # This function will set up the weights for problems with unbalanced data. \n",
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return  {cls: float(majority/count) for cls, count in counter.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "Through the course of this research multiple hyperparameter values were tested.  The code below represents the combination of hyperparameters and their corresponding values that provided the best results.  When experimenting with this architecture or adapting it to your own data you may want to alter the values shown here to see if you get better results.  Lets take a closer look at some of the hyperparameters and code shown in the next code block.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**ReduceLROnPlateau()**\n",
    "\n",
    "*lr_reducer = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=np.sqrt(0.1), cooldown=0, patience=2, min_lr=0.5e-6)*\n",
    "\n",
    "The learning rate represents how much we move in a certain direction in attempt to find the global minima.  Since initial weight values are somewhat arbitrary, starting with a higher learning rate typically works fine.  As we progress through the training phase, we typically find that we get closer and closer to either the global or local minima.  Since we do not want to overshoot the minima, a common practice is to anneal the learning rate.  In other words, as we progress through the training phase, we start to take smaller and smaller steps in a certain direction.  ReduceLROnPlateau() represents one example of how you can anneal the learning rate.  In the code example above, we will keep reducing the learning rate by the square root of 0.1 when there is no change in the loss value as long as the learning rate has not reached a reduction of 0.5e-6.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**EarlyStopping()**\n",
    "\n",
    "*early_stopper = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=10)*\n",
    "\n",
    "Early stopping mechanisms help prevent or limit overfitting models to training data.  Also, early stopping mechanisms prevent unnecessary computations when results remain static.  The model presented here will stop training if for a patience of 10 (epochs) there has not been a change of at least 0.001.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**CSVLogger**\n",
    "\n",
    "*csv_logger = CSVLogger('qp_logs.csv')*\n",
    "\n",
    "CSVLogger() allows us to bring in portions of our data into memory instead of having to keep all the data in memory.  Since memory is typically constrained during training, having this option is a necessity in many cases.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Batch Size**\n",
    "\n",
    "The batch size indicates the number of images that are processed during forward propagation to yield a loss value that is used in backpropagation.  Typically batch size is set to a power of two and is limited by the amount of memory available.  Also, though larger batch size may allow for faster training, weights update less frequently and may fail to provide the best results.\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "**Number of Epochs**\n",
    "\n",
    "The number of epochs represents the number of times the entire training dataset is iterated over when training your model.  Validation, performed at the end of each epoch, identifies how well the model generalizes to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For network monitoring\n",
    "# Will reduce the learning rate\n",
    "lr_reducer = ReduceLROnPlateau(monitor='val_categorical_accuracy', factor=np.sqrt(0.1), cooldown=0, patience=10, min_lr=1e-7)\n",
    "\n",
    "# if used will stop the training when the network stops improving for 10 epochs (patience)\n",
    "early_stopper = EarlyStopping(monitor='val_categorical_accuracy', min_delta=0.001, patience=10)\n",
    "\n",
    "\n",
    "# Fit paramters\n",
    "batch_size = 32\n",
    "\n",
    "# How many epochs to train\n",
    "nb_epoch = 10\n",
    "\n",
    "# Input image dimensions \n",
    "scan, img_rows, img_cols = 256, 256, 256\n",
    "img_channels = 1 # Gray scale images\n",
    "\n",
    "# Dataset locations \n",
    "# TrainingValidation='/home/m112447/Desktop/Python_projects/DEMO_CODE/DATA/1p19q/'+'n43_PQ_Part1'+str(scan)+'.npz'\n",
    "# Testing='/home/m112447/Desktop/Python_projects/DEMO_CODE/DATA/1p19q/'+'n43_PQ_Part2'+str(scan)+'.npz'\n",
    "TrainingValidation = os.path.join('/', 'data', 'interTCGA_part1'+str(scan)+'.npz')\n",
    "Testing = os.path.join('/', 'data', 'interTCGA_part2'+str(scan)+'.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Building A Model\n",
    "The code that follows creates an instance of the previously defined network and will use the previously defined hyperparameters values to build a model.  Code has been injected into various parts of the training phase to provide insight into how the model is progressing.  Also, some data preparation steps are included at the beginning of the following code block - removing a third class, normalizing the data, seting up cross-validation, etc.  Please take a moment to read through the following code before executing as the code reveals several approaches that can be taken in your own research to better understand both the data and the model produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Location to save the output of these code\n",
    "    output = os.path.join(os.getcwd(), '1p19qTCGA/')\n",
    "    if not os.path.exists(output):\n",
    "        os.makedirs(output)\n",
    "    csv_logger = CSVLogger(output+'qp_logs.csv')\n",
    "\n",
    "\n",
    "    # Load Trainig Validation        \n",
    "    X, y=load_set(path=TrainingValidation,scale=scan)\n",
    "    \n",
    "    # Data contain a third class normal --> Remove it for this demo    \n",
    "    IND=np.nonzero(y==2)\n",
    "    y = np.delete(y, IND)\n",
    "    X = np.delete(X, IND,axis=0)\n",
    "    Y_train=y\n",
    "    X_train=X\n",
    "    \n",
    "    # Delete temporary variables\n",
    "    del X, y\n",
    "    \n",
    "    # Preview an image\n",
    "    plt.figure()\n",
    "    plt.imshow(X_train[400,0,:,:], cmap=plt.cm.gray)\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    # Load Testing Validation        \n",
    "    X, y=load_set(path=Testing,scale=scan)\n",
    "    \n",
    "    # Data contain a third class normal --> Remove it for this demo    \n",
    "    IND=np.nonzero(y==2)\n",
    "    y = np.delete(y, IND)\n",
    "    X = np.delete(X, IND,axis=0)\n",
    "    y_test=y\n",
    "    X_test=X\n",
    "    \n",
    "    # Shuffle the training data   \n",
    "    Y_train,X_train = shuffle(Y_train,X_train, random_state=0)\n",
    "    print ('Training Validation Size')\n",
    "    print (np.shape(X_train))\n",
    "    print ('Testing size')\n",
    "    print (np.shape(X_test))\n",
    "    \n",
    "    # Normalize the data...the 10000 is set on the dataset creation process  \n",
    "    # All the data are T2 just used MRIcron to draw a line acrross Z direction of the tumor\n",
    "    X_train=X_train/10000\n",
    "    X_test=X_test/10000\n",
    "    \n",
    "    X_train=X_train.reshape(-1,img_rows, img_cols, img_channels)\n",
    "    X_test=X_test.reshape(-1,img_rows, img_cols, img_channels)\n",
    "    \n",
    "    model=cnn_model(img_rows, img_cols, img_channels)\n",
    "    print(ascii(model))\n",
    "    \n",
    "    # Save the model in case that needs to be reused\n",
    "    model_json = model.to_json()\n",
    "    #with open(output+'/'+\"FULL_MODEL.json\", \"w\") as json_file:\n",
    "    with open(os.path.join(output, 'FULL_MODEL.json'), 'w') as json_file:\n",
    "        json_file.write(model_json)\n",
    "    \n",
    "    # Evaluate the model using three-fold cross validation\n",
    "    kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)\n",
    "    \n",
    "    # Keep scores during testing in cross-validation phase\n",
    "    cvscores = []\n",
    "    rocval=[]\n",
    "    prediction=[]\n",
    "    recall=[]\n",
    "    f1=[]\n",
    "    \n",
    "    i=0\n",
    "    \n",
    "    # Keep scores during validation in cross-validation phase\n",
    "    cvscores_v = []\n",
    "    rocval_v=[]\n",
    "    prediction_v=[]\n",
    "    recall_v=[]\n",
    "    f1_v=[]\n",
    "    \n",
    "    for train, test in kfold.split(X_train, Y_train):\n",
    "        i+=1\n",
    "        \n",
    "        #Will produce a csv file with all the training history\n",
    "        csv_logger=CSVLogger(output+'/qp_logs'+str(i)+'.csv')\n",
    "        \n",
    "        model = cnn_model(img_rows, img_cols, img_channels)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['categorical_accuracy'])\n",
    "        \n",
    "        print ('Fold', str(i))\n",
    "        print (10*'----------')\n",
    "        print (10*'----------')\n",
    "        \n",
    "        # Estimate the weights for this fold\n",
    "        class_weight = get_class_weights(Y_train[train])\n",
    "        # Load a pretained model. \n",
    "        model.load_weights(os.path.join('/', 'data', 'pretrainedmodel.h5'))       \n",
    "        # Save the best model\n",
    "        best_model = ModelCheckpoint(output+'1p19q'+str(i)+'.h5', verbose=0, monitor='val_categorical_accuracy',save_best_only=True)\n",
    "        \n",
    "        #Perform the fitting\n",
    "        history=model.fit(X_train[train], np_utils.to_categorical(Y_train[train],2),\n",
    "              batch_size=batch_size,\n",
    "              epochs=nb_epoch,\n",
    "               validation_data=(X_train[test], np_utils.to_categorical(Y_train[test],2)),\n",
    "              shuffle=True,\n",
    "              callbacks=[lr_reducer, csv_logger,early_stopper,best_model],verbose=0)\n",
    "        \n",
    "        # Load the weights corresponding to the best model \n",
    "        model.load_weights(output+'1p19q'+str(i)+'.h5')\n",
    "        print ('Validation')\n",
    "        \n",
    "        # Estimate accuracy on validation set\n",
    "        print ('Overall accuracy: Validation')\n",
    "        scores = model.evaluate(X_train[test],  np_utils.to_categorical(Y_train[test],2), verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        cvscores_v.append(scores[1] * 100)\n",
    "        \n",
    "        # Plot loss curves\n",
    "        f = plt.figure()\n",
    "        plt.plot(history.history['loss'])\n",
    "        plt.plot(history.history['val_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'test'], loc='upper left')\n",
    "        plt.show()      \n",
    "        f.savefig(output+\"1p19q_curves\"+str(i)+\".pdf\")\n",
    "        \n",
    "        Y_cv_pred = model.predict(X_train[test], batch_size = 32)\n",
    "        Y_cv_pred=np.argmax(Y_cv_pred, axis=1)\n",
    "        Y_cv_pred=np.array(Y_cv_pred)\n",
    "        Y_cv_pred=Y_cv_pred.astype(int)\n",
    "        Y_cv_pred = np.squeeze(np.asarray(Y_cv_pred))\n",
    "        score1 = accuracy_score( Y_train[test], Y_cv_pred)\n",
    "        \n",
    "        print('Confusion matrix: Validation')\n",
    "        print(confusion_matrix( Y_train[test], Y_cv_pred))\n",
    "        target_names=[]\n",
    "        \n",
    "        print (\"The f1-score gives you the harmonic mean of precision and recall. The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes.The support is the number of samples of the true response that lie in that class.\")\n",
    "        target_names=['1p19q deleted', '1p19q not deleted' ]\n",
    "        \n",
    "        print(classification_report( Y_train[test], Y_cv_pred, target_names=target_names,digits=4))\n",
    "        valid_preds = model.predict(X_train[test], verbose=0)\n",
    "        prediction_v.append( (metrics.precision_score( Y_train[test], Y_cv_pred,average='weighted')))\n",
    "        recall_v.append((metrics.recall_score( Y_train[test], Y_cv_pred,average='weighted')))\n",
    "        f1_v.append(metrics.f1_score( Y_train[test], Y_cv_pred,average='weighted'))\n",
    "        \n",
    "        # Accuracy on the Test dataset.  (Not used during the test phase)\n",
    "        print ('Testing Score')\n",
    "        print ('-------------')\n",
    "        print ('-------------')\n",
    "        Y_cv_pred = model.predict(X_test, batch_size = 32)\n",
    "        Y_cv_pred=np.argmax(Y_cv_pred, axis=1)\n",
    "        \n",
    "        print ('Overall Accuracy')\n",
    "        score1 = accuracy_score(y_test, Y_cv_pred)\n",
    "        cvscores.append(scores[1] * 100)\n",
    "        \n",
    "        print (\"The f1-score gives you the harmonic mean of precision and recall. The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes.The support is the number of samples of the true response that lie in that class.\")\n",
    "        print(classification_report(y_test, Y_cv_pred, target_names=target_names,digits=4))\n",
    "        prediction.append( (metrics.precision_score(y_test, Y_cv_pred,average='weighted')))\n",
    "        recall.append((metrics.recall_score(y_test, Y_cv_pred,average='weighted')))\n",
    "        f1.append(metrics.f1_score(y_test, Y_cv_pred,average='weighted'))\n",
    "        \n",
    "        print('Confustion matrix: Test set')\n",
    "        print(confusion_matrix( y_test, Y_cv_pred))\n",
    "        np.set_printoptions(precision=2)\n",
    "        cmatr=confusion_matrix( y_test, Y_cv_pred)\n",
    "        np.set_printoptions(precision=2)\n",
    "        \n",
    "        # Plot the normalized confusion matrix and save the output\n",
    "        plot_confusion_matrix(cmatr, classes=['1p19q mutated', '1p19q not mutated' ], normalize=True,\n",
    "                      title='Normalized confusion matrix',savepdf=output+\"1p19qconfusionmatrix\"+str(i)+\".pdf\")\n",
    "        \n",
    "        print (10*'----------')\n",
    "        print (10*'----------')\n",
    "        \n",
    "        # Delete the model and history before the next fold\n",
    "        del model\n",
    "        del history\n",
    "    \n",
    "    print('Summary')\n",
    "    \n",
    "    #Validation metrics\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores_v), np.std(cvscores_v)))\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(prediction_v), np.std(prediction_v)))\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(recall_v), np.std(recall_v)))\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(f1_v), np.std(f1_v)))\n",
    "    \n",
    "    # Test set metrics\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(prediction), np.std(prediction)))\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(recall), np.std(recall)))\n",
    "    print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(f1), np.std(f1)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Build Before Deployment\n",
    "Once you believe you have captured the best model, then it is time to evaluate a final set of results using all the data to ensure that the model's precision, recall and accuracy are at their optimum.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final data utilizing all the data\n",
    "\n",
    "model = cnn_model(img_rows, img_cols, img_channels)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "      optimizer='Adam',\n",
    "      metrics=['categorical_accuracy'])\n",
    "\n",
    "print (10*'----------')\n",
    "print (10*'----------')\n",
    "\n",
    "# Estimate the weights\n",
    "class_weight = get_class_weights(Y_train)\n",
    "print('Weights')\n",
    "print (class_weight)\n",
    "\n",
    "# Save the best model\n",
    "best_model = ModelCheckpoint(output+'1p19q_full.h5', verbose=0, monitor='val_categorical_accuracy',save_best_only=True)\n",
    "model.load_weights(os.path.join('/', 'data', 'pretrainedmodel.h5'))       \n",
    "\n",
    "# Perform the fitting\n",
    "history=model.fit(X_train, np_utils.to_categorical(Y_train,2),\n",
    "      batch_size=batch_size,\n",
    "      epochs=nb_epoch,\n",
    "        validation_split=.3,\n",
    "      shuffle=True,\n",
    "      callbacks=[lr_reducer, csv_logger,early_stopper,best_model],verbose=0)\n",
    "\n",
    "# Plot loss curves\n",
    "f = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()      \n",
    "f.savefig(output+\"1p19q_FULL\"+str(i)+\".pdf\")\n",
    "\n",
    "# Load best weights \n",
    "model.load_weights(output+'1p19q_full.h5')\n",
    "\n",
    "# Perform analysis on the test set\n",
    "print ('Testing Score')\n",
    "print ('-------------')\n",
    "print ('-------------')\n",
    "Y_cv_pred = model.predict(X_test, batch_size = 32)\n",
    "Y_cv_pred=np.argmax(Y_cv_pred, axis=1)\n",
    "\n",
    "print ('Overall Accuracy')\n",
    "score1 = accuracy_score(y_test, Y_cv_pred)\n",
    "\n",
    "print (\"The f1-score gives you the harmonic mean of precision and recall. The scores corresponding to every class will tell you the accuracy of the classifier in classifying the data points in that particular class compared to all other classes.  The support is the number of samples of the true response that lie in that class.\")\n",
    "print(classification_report(y_test, Y_cv_pred, target_names=target_names,digits=4))\n",
    "prediction.append( (metrics.precision_score(y_test, Y_cv_pred,average='weighted')))\n",
    "recall.append((metrics.recall_score(y_test, Y_cv_pred,average='weighted')))\n",
    "f1.append(metrics.f1_score(y_test, Y_cv_pred,average='weighted'))\n",
    "\n",
    "print('Confustion matrix: Test set')\n",
    "print(confusion_matrix( y_test, Y_cv_pred))\n",
    "np.set_printoptions(precision=2)\n",
    "cmatr=confusion_matrix( y_test, Y_cv_pred)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot the normalized confusion matrix and save the output\n",
    "plot_confusion_matrix(cmatr, classes=target_names, normalize=True,\n",
    "              title='Normalized confusion matrix',savepdf=output+\"1p19qconfusionmatrixfull\"+str(i)+\".pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What Next\n",
    "Many ways to explore and possibly improve model:\n",
    "\n",
    "- Add additional layers to the network\n",
    "\n",
    "- Change the number of neurons in those layers\n",
    "\n",
    "- Change some of the hyperparameters in the network configuration like dropout or learning rate, etc.\n",
    "\n",
    "\n",
    "Time permitting, go back to the code above and experiment with some of these suggested changes to see if you can achieve better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
